import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

class InsightGenerator:
    def __init__(self, model_name="google/flan-t5-xl"):
        """
        Inicializa el generador de respuestas con un modelo optimizado para razonamiento.
        """
        try:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"Cargando modelo en {self.device}...")

            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)

            print("Modelo cargado correctamente.")

        except Exception as e:
            raise RuntimeError(f"Error al cargar el modelo: {str(e)}")

    def generate_response(self, user_question, sql_result):
        """
        Genera una respuesta en lenguaje humano a partir de la pregunta del usuario y el resultado SQL.

        Args:
            user_question (str): Pregunta original del usuario.
            sql_result (str | int | float): Resultado de la consulta SQL.

        Returns:
            str: Respuesta generada en lenguaje natural y explicativa.
        """
        if not user_question.strip():
            return "No se recibiÃ³ una pregunta vÃ¡lida del usuario."

        if not sql_result:
            return "No se encontraron datos para responder a la consulta."

        # ğŸ”¹ **Nuevo Prompt Mejorado para Razonamiento**
        prompt = f"""
Ejemplo 1:
Pregunta: "Â¿CuÃ¡nto vendimos en el sector de moda en enero de 2023?"
Resultado SQL: 15,340
Respuesta esperada: "Las ventas totales en el sector de moda durante enero de 2023 fueron de 15,340 unidades. Enero suele ser un mes con buenas ventas debido a las rebajas de temporada."

Ejemplo 2:
Pregunta: "Â¿CuÃ¡l fue la facturaciÃ³n total en la categorÃ­a de alimentos en diciembre de 2022?"
Resultado SQL: 48,950
Respuesta esperada: "En diciembre de 2022, la facturaciÃ³n total en la categorÃ­a de alimentos fue de 48,950 dÃ³lares. Este mes suele ser alto en ventas debido a la demanda de productos para las festividades."

Ejemplo 3:
Pregunta: "Â¿CuÃ¡ntas unidades se vendieron en la categorÃ­a de deportes en abril de 2022?"
Resultado SQL: 12,000
Respuesta esperada: "En abril de 2022, la categorÃ­a de deportes vendiÃ³ 12,000 unidades, lo que refleja un alto interÃ©s en el equipamiento deportivo con la llegada del buen clima."

Ahora responde la siguiente pregunta de manera similar pero no igual con tu propio razonamiento, razonando el contexto correctamente:
Pregunta: "{user_question}"
Resultado SQL: {sql_result}
Respuesta esperada:
"""

        try:
            # TokenizaciÃ³n
            inputs = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=True).to(self.device)

            # ğŸ”¹ **GeneraciÃ³n Optimizada**
            outputs = self.model.generate(
                inputs["input_ids"],
                max_length=150,  # Permite respuestas mÃ¡s largas
                do_sample=True,
                temperature=0.7,  # MÃ¡s control en la variabilidad
                top_p=0.9,
                num_return_sequences=1
            )

            # DecodificaciÃ³n del texto generado
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()

            return response if response else "No se pudo generar una respuesta vÃ¡lida."

        except Exception as e:
            return f"Error al generar la respuesta: {str(e)}"


# ğŸ”¥ **Ejemplo de Prueba**
if __name__ == "__main__":
    insight_gen = InsightGenerator()

    # ğŸ”¹ **Simulando Pregunta del Usuario y Respuesta SQL**
    user_question = "Â¿Cuales fueron las ventas del primer trimestre del 2019 de productos de tela?"
    sql_result = 1900  # Simulamos la respuesta SQL como una cifra

    # Generar la respuesta en lenguaje natural y explicativa
    response = insight_gen.generate_response(user_question, sql_result)

    print("\nğŸ—¨ï¸ **Respuesta Generada:**\n", response)
